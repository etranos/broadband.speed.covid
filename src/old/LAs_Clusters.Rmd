---
title: "LAs_clusters"
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_float: FALSE
knit: (function(inputFile, encoding) {
    rmarkdown::render(inputFile, encoding = encoding, output_dir = "../output")
  })
---

```{r settings, include = FALSE}
library(ggplot2)
library(tidyverse)
library(rprojroot)
library(lubridate)
library(knitr)
library(rprojroot)
library(nnet)
library(stargazer)
library(openair)
library(rgdal)
library(corrplot)
library(DescTools)
library(geosphere)
library(rgeos)
library(raster)

# This is the project path
path <- find_rstudio_root_file()
```

## Load cluster data

```{r include = FALSE}

# load the data created by Data_Spatial.Rmd and ts_clusters.Rmd
path.cluster <- paste(path, "/data/temp/clusters_nodiff.csv", sep = "")
clusters <- read_csv(path.cluster)
names(clusters) [1] <- "LAD19NM"

path.data <- paste(path, "/data/temp/TSbb19_20sp.csv", sep = "")
TSbb19_20sp <- read_csv(path.data)

#split out 2020 before join to clusters to create descriptives
TS2020 <- selectByDate(TSbb19_20sp, year = 2020) 
TS2019 <- selectByDate(TSbb19_20sp, year = 2019)

# LAD codes and names
# get LA 
# this is the heaviest version:
# (i) directly from the web
#la <- readOGR("http://geoportal1-ons.opendata.arcgis.com/datasets/b6d2e15801de45328b760a4f55d74318_0.geojson?outSR={%22latestWkid%22:3857,%22wkid%22:102100}")#, layer="OGRGeoJSON")
# or, (ii) from the json I saved locally
# path.json <- paste(path, "./data/raw/Local_Authority_Districts_(April_2019)_Boundaries_UK_BFE.geojson", sep = "")
# la <- readOGR(path.json)#, layer="OGRGeoJSON")
# UK BGC: 
la <- readOGR("https://opendata.arcgis.com/datasets/0e07a8196454415eab18c40a54dfbbef_0.geojson")
codes <- la@data %>%
  dplyr::select(lad19cd, lad19nm) %>%
  rename(LAD19NM = lad19nm)

# LAD broadband characteristics AM
LAD.up.am19 <- TS2019 %>%
  filter(hour == 9 | hour == 10) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.up.am19 = mean(speedup))

LAD.up.am20 <- TS2020 %>%
  filter(hour == 9 | hour == 10) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.up.am20 = mean(speedup))

LAD.down.am19 <- TS2019 %>%
  filter(hour == 9 | hour == 10) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.down.am19 = mean(speeddown))

LAD.down.am20 <- TS2020 %>%
  filter(hour == 9 | hour == 10) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.down.am20 = mean(speeddown))

LAD.tests.am19 <- TS2019 %>%
  filter(hour == 9 | hour == 10) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.tests.am19 = n())

LAD.tests.am20 <- TS2020 %>%
  filter(hour == 9 | hour == 10) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.tests.am20 = n())

# LAD broadband characteristics PM
LAD.up.pm19 <- TS2019 %>%
  filter(hour == 19 | hour == 20) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.up.pm19 = mean(speedup))

LAD.up.pm20 <- TS2020 %>%
  filter(hour == 19 | hour == 20) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.up.pm20 = mean(speedup))

LAD.down.pm19 <- TS2019 %>%
  filter(hour == 19 | hour == 20) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.down.pm19 = mean(speeddown))

LAD.down.pm20 <- TS2020 %>%
  filter(hour == 19 | hour == 20) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.down.pm20 = mean(speeddown))

LAD.tests.pm19 <- TS2019 %>%
  filter(hour == 19 | hour == 20) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.tests.pm19 = n())

LAD.tests.pm20 <- TS2020 %>%
  filter(hour == 19 | hour == 20) %>%
  group_by(LAD19NM) %>%
  summarise(LAD.tests.pm20 = n())

# this is a share of Virgin provider variable
virgin <- TS2020 %>%
  filter(provider == "Virgin Media") %>%
  group_by(LAD19NM) %>%
  summarise(virgin = n())

virgin.denom <- TS2020 %>%
  #filter(provider == "Virgin Media") %>%
  group_by(LAD19NM) %>%
  summarise(virgin.denom = n())

virgin <- merge(virgin.denom, virgin, by = "LAD19NM", all.x = T)
virgin$virgin <- ifelse(is.na(virgin$virgin), 0, virgin$virgin)
virgin$virgin <- virgin$virgin/virgin.denom$virgin.denom
virgin$virgin.denom <- NULL

# LAD characteristcs merge
lad <- list(codes,
            clusters,
            LAD.up.am19,
            LAD.up.am20,
            LAD.down.am19,
            LAD.down.am20,
            LAD.tests.am19,
            LAD.tests.am20,
            LAD.up.pm19,
            LAD.up.pm20,
            LAD.down.pm19,
            LAD.down.pm20,
            LAD.tests.pm19,
            LAD.tests.pm20,
            virgin)

sapply(lad, function(x) dim(x))
sapply(lad, function(x) sum(is.na(x)))

# merge with reduce
lad <- lad %>% 
  reduce(inner_join, by = "LAD19NM")
sapply(lad, function(x) sum(is.na(x)))
```

## Distance to Metro Area

```{r}
# get LA 
# (i) directly from the web
#la <- readOGR("http://geoportal1-ons.opendata.arcgis.com/datasets/b6d2e15801de45328b760a4f55d74318_0.geojson?outSR={%22latestWkid%22:3857,%22wkid%22:102100}")#, layer="OGRGeoJSON")
# or, (ii) from the json I saved locally
path.json <- paste(path, "./data/raw/Local_Authority_Districts_(April_2019)_Boundaries_UK_BFE.geojson", sep = "")
la <- readOGR(path.json)#, layer="OGRGeoJSON")
# source: https://data.gov.uk/dataset/7c387c64-d25f-474a-b07e-b933578caea2/local-authority-districts-april-2019-boundaries-uk-bfe

# spatial transformations
la <- spTransform(la, CRS("+init=epsg:4326"))
la@data$LAD19NM <- as.character(la@data$LAD19NM)
ladDist <- la@data[,c("LAD19CD", "LAD19NM")]
names(ladDist)[1] <- "lad19cd"

#distance from London and other 
coords_LAD <- la@data[,c("LONG", "LAT")]
London <- la[which(la@data$LAD19NM == "City of London"),]
London <- gCentroid(London)
distLondon <- distm(coords_LAD, London, fun=distCosine) 
#result in meters - change to km
distLondon <- round((distLondon/1000),2)
Birmingham <- la[which(la@data$LAD19NM == "Birmingham"),]
Birmingham <- gCentroid(Birmingham)
distBham <- distm(coords_LAD, Birmingham, fun=distCosine)
distBham <- round((distBham/1000),2)
Manchester <- la[which(la@data$LAD19NM == "Manchester"),]
Manchester <- gCentroid(Manchester)
distManc <- distm(coords_LAD, Manchester, fun=distCosine)
distManc <- round((distManc/1000),2)
Leeds <- la[which(la@data$LAD19NM == "Leeds"),]
Leeds <- gCentroid(Leeds)
distLeeds <- distm(coords_LAD, Leeds, fun=distCosine)
distLeeds <- round((distLeeds/1000),2)
Liverpool <- la[which(la@data$LAD19NM == "Liverpool"),]
Liverpool <- gCentroid(Liverpool)
distLivp <- distm(coords_LAD, Liverpool, fun=distCosine)
distLivp <- round((distLivp/1000),2)
Newcastle <- la[which(la@data$LAD19NM == "Newcastle upon Tyne"),]
Newcastle <- gCentroid(Newcastle)
distNewc <- distm(coords_LAD, Newcastle, fun=distCosine)
distNewc <- round((distNewc/1000),2)
Sheffield <- la[which(la@data$LAD19NM == "Sheffield"),]
Sheffield <- gCentroid(Sheffield)
distShef <- distm(coords_LAD, Sheffield, fun=distCosine)
distShef <- round((distShef/1000),2)
SouthHants <- la[which(la@data$LAD19NM == "Fareham"),]
SouthHants <- gCentroid(SouthHants)
distSHam <- distm(coords_LAD, SouthHants, fun=distCosine)
distSHam <- round((distSHam/1000),2)
Nottingham <- la[which(la@data$LAD19NM == "Nottingham"),]
Nottingham <- gCentroid(Nottingham)
distNotts <- distm(coords_LAD, Nottingham, fun=distCosine)
distNotts <- round((distNotts/1000),2)
Bristol <- la[which(la@data$LAD19NM == "Bristol, City of"),]
Bristol <- gCentroid(Bristol)
distBris <- distm(coords_LAD, Bristol, fun=distCosine)
distBris <- round((distBris/1000),2)
Cardiff <- la[which(la@data$LAD19NM == "Cardiff"),]
Cardiff <- gCentroid(Cardiff)
distCard <- distm(coords_LAD, Cardiff, fun=distCosine)
distCard <- round((distCard/1000),2)
Glasgow <- la[which(la@data$LAD19NM == "Glasgow City"),]
Glasgow <- gCentroid(Glasgow)
distGlas <- distm(coords_LAD, Glasgow, fun=distCosine) 
distGlas <- round((distGlas/1000),2)

ladDist <- cbind(ladDist, distLondon, distBham, distManc, distLeeds,
                 distLivp, distNewc, distShef, distSHam, distNotts, 
                 distBris, distCard, distGlas)

ladDist$distMet <- apply(ladDist[,3:14], 1, min)
ladDist <- ladDist %>%
  dplyr::select(lad19cd, distLondon, distMet)


#test <- left_join(lad, ladDist)
```

## Job density 2018

```{r}

job.dens <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_57_1.data.csv?geography=1816133633...1816133848,1820327937...1820328318&date=latest&item=3&measures=20100")

job.dens <- job.dens %>%
  dplyr::select(GEOGRAPHY_CODE, GEOGRAPHY_NAME, OBS_VALUE) %>%
  rename(job.dens2018 = OBS_VALUE) %>%
  rename(lad19cd = GEOGRAPHY_CODE) %>%
  distinct()

# test <- left_join(lad, job.dens, by = "lad19cd")
```

## Population 2018

```{r}
# pop <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_31_1.data.csv?geography=1807745025...1807745028,1807745030...1807745032,1807745034...1807745155,1807745157...1807745164,1807745166...1807745170,1807745172...1807745177,1807745179...1807745194,1807745196,1807745197,1807745199,1807745201...1807745218,1807745221,1807745222,1807745224,1807745226...1807745231,1807745233,1807745234,1807745236...1807745244,1807745271...1807745281,1811939329...1811939332,1811939334...1811939336,1811939338...1811939497,1811939499...1811939501,1811939503,1811939505...1811939507,1811939509...1811939517,1811939519,1811939520,1811939524...1811939570,1811939575...1811939599,1811939601...1811939628,1811939630...1811939634,1811939636...1811939647,1811939649,1811939655...1811939664,1811939667...1811939680,1811939682,1811939683,1811939685,1811939687...1811939704,1811939707,1811939708,1811939710,1811939712...1811939717,1811939719,1811939720,1811939722...1811939730,1811939757...1811939767&date=latestMINUS1-latest&sex=7&age=0,22&measures=20100")

pop <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_31_1.data.csv?geography=1820327937...1820328318,1816133633...1816133848&date=latestMINUS1-latest&sex=7&age=0,22&measures=20100")

pop <- pop %>%
  dplyr::select(GEOGRAPHY_CODE, GEOGRAPHY_NAME, AGE_NAME, OBS_VALUE) %>%
  distinct(GEOGRAPHY_CODE, AGE_NAME, .keep_all = T) %>% 
  spread(AGE_NAME, OBS_VALUE) %>% 
  rename(pop16_64 = `Aged 16 - 64`) %>%
  rename(pop = `All ages`) %>%
  rename(lad19cd = GEOGRAPHY_CODE)

# test <- left_join(lad, pop, by = "lad19cd")
# sapply(test, function(x) sum(is.na(x)))
```

## Population density 2018

```{r}

# this is just population
pop.dens <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_31_1.data.csv?geography=1820327937...1820328318,1816133633...1816133848&date=latestMINUS1-latest&sex=7&age=0,22&measures=20100")

pop.dens <- pop.dens %>%
  dplyr::select(GEOGRAPHY_CODE, GEOGRAPHY_NAME, AGE_NAME, OBS_VALUE) %>%
  distinct(GEOGRAPHY_CODE, AGE_NAME, .keep_all = T) %>% 
  spread(AGE_NAME, OBS_VALUE) %>% 
  rename(pop16_64 = `Aged 16 - 64`) %>%
  rename(pop = `All ages`) %>%
  rename(lad19cd = GEOGRAPHY_CODE) %>%
  dplyr::select(-pop16_64)

crs(la)
la$area_sqkm <- area(la) / 1000000

pop.dens <- merge(pop.dens, la@data, by.x = "lad19cd", by.y = "LAD19CD", all.x = T)

pop.dens <- pop.dens %>%
  mutate(pop.dens = pop/area_sqkm) %>%
  dplyr::select(lad19cd,pop.dens)

# sapply(pop.dens, function(x) sum(is.na(x)))
# test <- pop.dens[is.na(pop.dens$area_sqkm),]
```

## N/S divide

```{r}

# It only includes only the south
 
ns <- read_csv("https://opendata.arcgis.com/datasets/3ba3daf9278f47daba0f561889c3521a_0.csv") %>%
  mutate(south = ifelse(RGN19NM=="London" | 
                        RGN19NM=="South West" |
                        RGN19NM=="East of England" |
                        RGN19NM=="South East", 1, 0)) %>%
  dplyr::select(LAD19CD, south) %>% 
  dplyr::filter(south==1) %>%
  rename(lad19cd = LAD19CD)
```

## Labour supply March 2020

```{r}

labour <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_17_5.data.csv?geography=1816133633...1816133837,1820327937...1820328307&date=latest&variable=18,1532...1540,290,720&measures=20599,21001,21002,21003")

labour <- labour %>%
  dplyr::select(GEOGRAPHY_CODE, GEOGRAPHY_NAME, VARIABLE_NAME, OBS_VALUE) %>%
  distinct(GEOGRAPHY_CODE, VARIABLE_NAME, .keep_all = T) %>% 
  spread(VARIABLE_NAME, OBS_VALUE) %>% 
  rename(lad19cd = GEOGRAPHY_CODE) %>%
  rename(managers =  `% all in employment who are - 1: managers, directors and senior officials (SOC2010)`) %>%
  rename(prof = `% all in employment who are - 2: professional occupations (SOC2010)`) %>%
  rename(tech = `% all in employment who are - 3: associate prof & tech occupations (SOC2010)`) %>%
  rename(admin = `% all in employment who are - 4: administrative and secretarial occupations (SOC2010)`) %>%
  rename(skilled = `% all in employment who are - 5: skilled trades occupations (SOC2010)`) %>%
  rename(caring = `% all in employment who are - 6: caring, leisure and other service occupations (SOC2010)`) %>%
  rename(sales = `% all in employment who are - 7: sales and customer service occupations (SOC2010)`) %>%
  rename(plant = `% all in employment who are - 8: process, plant and machine operatives (SOC2010)`) %>%
  rename(elementary = `% all in employment who are - 9: elementary occupations (SOC2010)`) %>%
  rename(NVQ3 = `% with NVQ3+ - aged 16-64`) %>%
  rename(NVQ4 = `% with NVQ4+ - aged 16-64`) %>%
  rename(econ.act = `Economic activity rate - aged 16-64`) %>% # No NVQ data here as this is for Jul 2019-Jun 2020. NVQ is only  
  dplyr::select(-NVQ3, -NVQ4)                                  # available for Dec - Dec. I download these data seperetely.

# test <- left_join(lad, labour, by = "lad19cd")
# sapply(test, function(x) sum(is.na(x)))
```

## Labour supply March 2020

```{r}

nvq4 <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_17_5.data.csv?geography=1811939329...1811939332,1811939334...1811939336,1811939338...1811939497,1811939499...1811939501,1811939503,1811939505...1811939507,1811939509...1811939517,1811939519,1811939520,1811939524...1811939570,1811939575...1811939599,1811939601...1811939628,1811939630...1811939634,1811939636...1811939647,1811939649,1811939655...1811939664,1811939667...1811939680,1811939682,1811939683,1811939685,1811939687...1811939704,1811939707,1811939708,1811939710,1811939712...1811939717,1811939719,1811939720,1811939722...1811939730,1807745025...1807745028,1807745030...1807745032,1807745034...1807745155,1807745157...1807745164,1807745166...1807745170,1807745172...1807745177,1807745179...1807745194,1807745196,1807745197,1807745199,1807745201...1807745218,1807745221,1807745222,1807745224,1807745226...1807745231,1807745233,1807745234,1807745236...1807745244&date=latestMINUS2&variable=290&measures=20599,21001,21002,21003")

nvq4 <- nvq4 %>%
  dplyr::select(GEOGRAPHY_CODE, GEOGRAPHY_NAME, MEASURES_NAME, OBS_VALUE) %>%
  filter(MEASURES_NAME=="Variable") %>%
  dplyr::select(-MEASURES_NAME) %>%
  distinct(GEOGRAPHY_CODE, OBS_VALUE, .keep_all = T) %>% 
  rename(lad19cd = GEOGRAPHY_CODE) %>%
  rename(nvq4 = OBS_VALUE)
```

## Earnings 2019

```{r}

earnings <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_30_1.data.csv?geography=1946157348,2092957698,2013265929,1816133633...1816133837,1820327937...1820328307&date=latest&sex=8&item=2&pay=1&measures=20100,20701")

earnings <- earnings %>%
  filter(MEASURES_NAME == "Value") %>% # dropiing the confedence
  dplyr::select(GEOGRAPHY_CODE, GEOGRAPHY_NAME, OBS_VALUE) %>%
  rename(lad19cd = GEOGRAPHY_CODE) %>%
  rename(earnings =  OBS_VALUE) %>%
  distinct()

# test <- left_join(lad, earnings, by = "lad19cd")
# sapply(test, function(x) sum(is.na(x)))
```

## Business counts 2019

```{r}

# busi <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_141_1.data.csv?geography=1816133633...1816133848,1820327937...1820328318,1870659585...1870659791,1870659801,1870659792...1870659800,1879048193...1879048573,1879048583,1879048574...1879048582&date=latest&industry=163577857...163577874&employment_sizeband=0&legal_status=0&measures=20100")

busi <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_141_1.data.csv?geography=1816133633...1816133848,1820327937...1820328318,1870659585...1870659791,1870659801,1870659792...1870659800,1879048193...1879048573,1879048583,1879048574...1879048582&date=latest&industry=150994945...150994965&employment_sizeband=0&legal_status=0&measures=20100")

busi <- busi %>%
  dplyr::select(GEOGRAPHY_CODE, GEOGRAPHY_NAME, INDUSTRY_NAME, OBS_VALUE) %>%
  distinct(GEOGRAPHY_CODE, INDUSTRY_NAME, .keep_all = T) %>% 
  spread(INDUSTRY_NAME, OBS_VALUE) %>% 
  rename(lad19cd = GEOGRAPHY_CODE) %>%
  rename(A = `A : Agriculture, forestry and fishing`) %>%
  rename(B = `B : Mining and quarrying`) %>%
  rename(C = `C : Manufacturing`) %>%
  rename(D = `D : Electricity, gas, steam and air conditioning supply`) %>%
  rename(E = `E : Water supply; sewerage, waste management and remediation activities`) %>%
  rename(F = `F : Construction`) %>%
  rename(G = `G : Wholesale and retail trade; repair of motor vehicles and motorcycles`) %>%
  rename(H = `H : Transportation and storage`) %>%
  rename(I = `I : Accommodation and food service activities`) %>%
  rename(J = `J : Information and communication`) %>%
  rename(K = `K : Financial and insurance activities`) %>%
  rename(L = `L : Real estate activities`) %>%
  rename(M = `M : Professional, scientific and technical activities`) %>%
  rename(N = `N : Administrative and support service activities`) %>%
  rename(O = `O : Public administration and defence; compulsory social security`) %>%
  rename(P = `P : Education`) %>%
  rename(Q = `Q : Human health and social work activities`) %>%
  rename(R = `R : Arts, entertainment and recreation`) %>%
  rename(S = `S : Other service activities`) %>%
  rename(T = `T : Activities of households as employers;undifferentiated goods-and services-producing activities of households for own use`) %>%
  rename(U = `U : Activities of extraterritorial organisations and bodies`) %>%
  mutate(total.busi = rowSums(.[3:23]))

# test <- left_join(lad, busi, by = "lad19cd")
# sapply(test, function(x) sum(is.na(x)))
```

## Urban/rural

```{r eval=F}
# not complete
url <- "https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/137663/la-class-dataset-post0409.xls"
GET(url, write_disk(tf <- tempfile(fileext = ".xls")))
urban.rural <- read_excel(tf, 1L, range = "A17:CP1877", col_names = T)

```

## merge

```{r}
data <- list(lad, ladDist, job.dens, pop, pop.dens, labour, earnings, busi, nvq4) # ns 
sapply(data, function(x) dim(x))
sapply(data, function(x) names(x))

# merge with reduce
data <- data %>% 
  reduce(inner_join, by = "lad19cd") %>%
  #distinct(.keep_all = T)
  unique()
sapply(data, function(x) sum(is.na(x)))

data <- merge(data, ns, all.x = T)
data$south <- ifelse(is.na(data$south),0, data$south)

# NOT RUN
# export file for future reference / backup
# data.out.path <- paste0(path, "/data/temp/data_for_aux.csv")
# write.csv(data, data.out.path)
```

## Descriptive statistics and correlations

There is surprisingly low correlation between upload and download, morning and night. 
Number of tests are the most correlated variables.

```{r correlations, results = 'asis'}
data.cor <- data %>%
  dplyr::select(LAD.up.am19, LAD.down.am19,  
                LAD.tests.am19, LAD.up.pm19, 
                LAD.down.pm19, LAD.tests.pm19, 
                LAD.up.am20, LAD.down.am20,
                LAD.tests.am20, LAD.up.pm20, 
                LAD.down.pm20, LAD.tests.pm20)

m <- cor(data.cor)
corrplot(m, type="upper",method = "number", number.cex = .5, tl.cex = .75)

data.cor <- data %>%
  dplyr::select(LAD.tests.am20,
                pop, pop16_64,
                managers, tech,
                skilled, earnings,
                total.busi)


data.cor <- data %>%
  dplyr::select(pop16_64, 
                pop.dens,
                job.dens2018, 
                distMet,
                distLondon,
                south,
                managers, 
                tech,
                skilled,
                prof,
                admin, 
                caring,
                plant,
                earnings,
                total.busi,
                LAD.tests.am20,
                pop,
                virgin) %>%
  mutate('LAD.tests.am20/pop' = LAD.tests.am20/pop) %>%
    mutate('total.busi/pop' = total.busi/pop) %>%
  dplyr::select(-LAD.tests.am20,-pop, -total.busi)


m <- cor(data.cor, use = "pairwise.complete.obs")
corrplot(m, type="upper",method = "number", number.cex = .5, tl.cex = .75)

stargazer(data.cor,
          type="html",
          #type="text", 
          summary = T,
          nobs = T)
```

## Auxiliary regression, dep. var.: upload clusters

```{r upload regression , results= 'asis', message=F, error=F, warning=F}
# helpful sources: http://www.princeton.edu/~otorres/LogitR101.pdf
# https://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/

# convert dependent variable to factor
data$cluster.up <- as.factor(data$cluster.up)
data$cluster.up <- relevel(data$cluster.up, "4") # base category: cluster 4

aux.up <- multinom(cluster.up ~ 
                  #pop16_64 + # non sign, while pop is
                  pop +
                  #pop.dens + 
                  job.dens2018 + # pop.dens is still insignificant when job.dens is excluded
                  distMet + distLondon + south +
                  managers + tech + skilled + prof + admin + caring + plant +
                  earnings + I(total.busi/pop) + nvq4 +
                  I(LAD.tests.am20/pop) + # non sensitive to addition of other tests pm
                  virgin
                , data = data, 
                trace = F) # to avoid messages 

# McFadden's R squared
aux.up.null <- multinom(cluster.up ~ 1, data = data)
r2.up <- 1-logLik(aux.up)/logLik(aux.up.null)

n.obs <- dim(aux.up$fitted.values)[1]

stargazer(aux.up, 
          #type = "html",
          type = "text",
          nobs = T,
          digits = 3,
          covariate.labels=c(#"pop. 16-64, 2018",
                             "pop, 2018",
                             #"pop. densuty, 2018",
                             "job density, 2018",
                             "distance to nearest met. area",
                             "distance to London",
                             "South of the UK",
                             "% of managerial jobs, 2020",
                             "% of tech jobs, 2020",
                             "% of skilled trade jobs, 2020",
                             "% of professional jobs, 2020",
                             "% of administrative jobs, 2020",
                             "% of leisure jobs, 2020",
                             "% of machine operation jobs, 2020",
                             "earnings, 2019",
                             "n. business est. per hab., 2019",
                             "% of NVQ4+",
                             "n. of AM broadband tests per hab., 2020",
                             "% of Virgin Media connections"),
          add.lines = list(c("McFadden's R squared",
                             round(r2.up[1],3), round(r2.up[1],3), round(r2.up[1],3),
                             round(r2.up[1],3), round(r2.up[1],3), round(r2.up[1],3),
                             round(r2.up[1],3), round(r2.up[1],3), round(r2.up[1],3)
                             , round(r2.up[1],3), round(r2.up[1],3)),
                           (c("N",
                            n.obs, n.obs, n.obs,
                            n.obs, n.obs, n.obs,
                            n.obs, n.obs, n.obs
                            , n.obs, n.obs))))

```

## Auxiliary regression, dep. var.: test clusters

```{r test regression, results= 'asis', message=F, error=F, warning=F}
# helpful sources: http://www.princeton.edu/~otorres/LogitR101.pdf
# https://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/

# convert dependent variable to factor
data$cluster.tests <- as.factor(data$cluster.tests)
data$cluster.tests <- relevel(data$cluster.tests, "4") # base category

aux.tests <- multinom(cluster.tests ~ 
                  #pop16_64 + # non sign, while pop is
                  pop +
                  #pop.dens + 
                  job.dens2018 + # pop.dens is still insignificant when job.dens is excluded
                  distMet + distLondon + south + nvq4 +
                  managers + tech + skilled + prof + admin + caring + plant +
                  earnings + I(total.busi/pop) +
                  I(LAD.tests.am20/pop) + # non sensitive to addition of other tests pm
                  virgin
                , data = data, 
                trace = F) # to avoid messages 

# McFadden's R squared
aux.tests.null <- multinom(cluster.tests ~ 1, data = data)
r2.tests <- 1-logLik(aux.tests)/logLik(aux.tests.null)

n.obs <- dim(aux.tests$fitted.values)[1]

stargazer(aux.tests, 
          type = "html",
          #type = "text",
          digits = 3,
          covariate.labels=c(#"pop. 16-64, 2018",
                             "pop, 2018",
                             #"pop. densuty, 2018",
                             "job density, 2018",
                             "distance to nearest met. area",
                             "distance to London",
                             "South of the UK",
                             "% of managerial jobs, 2020",
                             "% of tech jobs, 2020",
                             "% of skilled trade jobs, 2020",
                             "% of professional jobs, 2020",
                             "% of administrative jobs, 2020",
                             "% of leisure jobs, 2020",
                             "% of machine operation jobs, 2020",
                             "earnings, 2019",
                             "n. business est. per hab., 2019",
                             "% of NVQ4+",
                             "n. of AM broadband tests per hab., 2020",
                             "% of Virgin Media connections"),
          add.lines = list(c("McFadden's R squared",
                             round(r2.tests[1],3), round(r2.tests[1],3), round(r2.tests[1],3),
                             round(r2.tests[1],3), round(r2.tests[1],3), round(r2.tests[1],3)),
                           (c("N",
                            n.obs, n.obs, n.obs,
                            n.obs, n.obs, n.obs))))

```

## Auxiliary regression, dep. var.: down clusters

```{r down regression, results= 'asis', message=F, error=F, warning=F}
# helpful sources: http://www.princeton.edu/~otorres/LogitR101.pdf
# https://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/

# convert dependent variable to factor
data$cluster.down <- as.factor(data$cluster.down)
data$cluster.down <- relevel(data$cluster.down, "2") # base category

aux.down <- multinom(cluster.down ~ 
                  #pop16_64 + # non sign, while pop is
                  pop +
                  #pop.dens + 
                  job.dens2018 + # pop.dens is still insignificant when job.dens is excluded
                  distMet + distLondon + south +
                  managers + tech + skilled + prof + admin + caring + plant +
                  earnings + I(total.busi/pop) + nvq4 +
                  I(LAD.tests.am20/pop) + # non sensitive to addition of other tests pm
                  virgin
                , data = data, 
                trace = F) # to avoid messages 

# McFadden's R squared
aux.down.null <- multinom(cluster.down ~ 1, data = data)
r2.down <- 1-logLik(aux.down)/logLik(aux.down.null)

n.obs <- dim(aux.down$fitted.values)[1]

stargazer(aux.down, 
          type = "html",
          #type = "text",
          digits = 3,
          covariate.labels=c(#"pop. 16-64, 2018",
                             "pop, 2018",
                             #"pop. densuty, 2018",
                             "job density, 2018",
                             "distance to nearest met. area",
                             "distance to London",
                             "South of the UK",
                             "% of managerial jobs, 2020",
                             "% of tech jobs, 2020",
                             "% of skilled trade jobs, 2020",
                             "% of professional jobs, 2020",
                             "% of administrative jobs, 2020",
                             "% of leisure jobs, 2020",
                             "% of machine operation jobs, 2020",
                             "earnings, 2019",
                             "n. business est. per hab., 2019",
                             "% of NVQ4+",
                             "n. of AM broadband tests per hab., 2020",
                             "% of Virgin Media connections"),
          add.lines = list(c("McFadden's R squared",
                             round(r2.down[1],3), round(r2.down[1],3), round(r2.down[1],3),
                             round(r2.down[1],3), round(r2.down[1],3), round(r2.down[1],3),
                             round(r2.down[1],3), round(r2.down[1],3), round(r2.down[1],3),
                             round(r2.down[1],3), round(r2.down[1],3), round(r2.down[1],3),
                             round(r2.down[1],3), round(r2.down[1],3), round(r2.down[1],3),
                             round(r2.down[1],3), round(r2.down[1],3), round(r2.down[1],3),
                             round(r2.down[1],3)),
                           (c("N",
                            n.obs, n.obs, n.obs,
                            n.obs, n.obs, n.obs,
                            n.obs, n.obs, n.obs,
                            n.obs, n.obs, n.obs,
                            n.obs, n.obs, n.obs,
                            n.obs, n.obs, n.obs,
                            n.obs))))

```

```{r}
path.out <- paste0(path, "/data/temp/LAs_Clusters.RData")
save.image(path.out)
```